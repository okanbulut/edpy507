---
title: "Item Response Theory (IRT)"
output:
  html_document:
    toc: TRUE
bibliography: [mybib.bib, packages_irt.bib]
csl: apa.csl
---


```{r setup, include = FALSE}
#Setup knitr
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, background = "gray85", message = FALSE, fig.width=8, fig.height=6, comment = NA, fig.align = 'center')

suppressWarnings({
  library("rmarkdown")
  library("fontawesome")
  library("kableExtra")
  library("emo")
  
  # Required packages
  library("DataExplorer")
  library("ggcorrplot")
  library("psych")
  library("lavaan")
  library("mirt")
  library("ShinyItemAnalysis")
})

# automatically create a bib database for R packages
knitr::write_bib(x = c(.packages()), file = "packages_irt.bib")
```

------------------------------------------------------------------------

# Example: Synthetic Aperture Personality Assessment

The Synthetic Aperture Personality Assessment (SAPA) is a web based personality assessment project (<https://www.sapa-project.org/>). The purpose of SAPA is to find patterns among the vast number of ways that people differ from one another in terms of their thoughts, feelings, interests, abilities, desires, values, and preferences [@condon2014; @revelle2010]. In this example, we will use a subset of SAPA (16 items) sampled from the full instrument (80 items) to develop online measures of ability. These 16 items measure four subskills (i.e., verbal reasoning, letter series, matrix reasoning, and spatial rotations) as part of the general intelligence, also known as g factor. The "sapa.csv" dataset is a data frame with 1525 individuals who responded to 16 multiple-choice items in SAPA. The original dataset is included in the **psych** package [@R-psych]. The dataset can be downloaded from [**here**](data_and_codes/sapa.csv). In addition, the R codes for the item response theory (IRT) analyses presented on this page are available [**here**](data_and_codes/irt.R).

<br>

## Setting up R

In our example, we will conduct IRT and other relevant analyses using the following packages:

```{r irt1, eval=TRUE, echo=FALSE}
pkg_dat <- data.frame(
  Package = c("DataExplorer", "ggcorrplot", "psych", "lavaan", "mirt", "ShinyItemAnalysis"),
  URL = c("http://CRAN.R-project.org/package=DataExplorer",
          "http://CRAN.R-project.org/package=ggcorrplot",
          "http://CRAN.R-project.org/package=psych",
          "http://CRAN.R-project.org/package=lavaan",
          "http://CRAN.R-project.org/package=mirt",
          "http://CRAN.R-project.org/package=ShinyItemAnalysis")
)

kbl(pkg_dat, caption = "") %>%
  kable_paper("striped") %>%
  pack_rows("Exploratory Data Analysis", 1, 2) %>%
  pack_rows("Psychometric Analysis", 3, 6)
```

<br>

We have already installed and used some of the above packages in the [CTT](https://okanbulut.github.io/edpy507/ctt.html) section. Therefore, we will only install the new R packages (**lavaan** and **mirt**) at this time and then activate all the required packages one by one using the `library()` command:


```{r irt2, eval=FALSE}
# Install the missing packages
install.packages(c("lavaan", "mirt"))

# Activate the required packages
library("DataExplorer")
library("ggcorrplot")
library("psych")
library("lavaan")
library("mirt")
library("ShinyItemAnalysis")
```

We will use **lavaan** [@R-lavaan] to conduct confirmatory factor analysis and **mirt** [@R-mirt] to estimate dichotomous and polytomous IRT models.

***

> `r emo::ji("bell")` [**INFORMATION:**]{style="color:blue"} There are many other packages for estimating IRT models in R, such as **ltm** [@R-ltm], **eRm** [@R-eRm], **TAM** [@R-TAM], and **irtoys** [@R-irtoys]. I prefer the **mirt** package because it includes functions to estimate various IRT models (e.g., unidimensional, multidimensional, and explanatory IRT models), additional functions to check model assumptions (e.g., local independence), and various tools to visualize IRT-related objects (e.g., item characteristic curve, item information function, and test information function). You can check out @irtpkg for a detailed review of IRT packages available in R.

***

## Exploratory data analysis

We will begin our analysis by conducting [exploratory data analysis (EDA)](https://okanbulut.github.io/bigdata/eda.html). As you may remember from the [CTT](https://okanbulut.github.io/edpy507/ctt.html) section, we use EDA to check the quality of our data and identify potential problems (i.e., missing values) in the data. In this section, we will import [sapa.csv](data_and_codes/sapa.csv) into R and review the variables in the dataset using descriptive statistics and visualizations. 

First, we need to set up our working directory. I have created a new folder called "IRT Analysis" on my desktop and put our data ([sapa_data.csv](data_and_codes/sapa_data.csv)) into this folder. Now, we will change our working directory to this new folder (use the path where you keep these files in your own computer):

```{r irt3, eval=FALSE}
setwd("C:/Users/Okan/Desktop/IRT Analysis")
```

Next, we will import the data into R using the `read.csv()` function and save it as "sapa".

```{r irt4, eval=FALSE}
sapa <- read.csv("sapa_data.csv", header = TRUE)
```

Using the `head()` function, we can now view the first 6 rows of the `sapa` dataset:

```{r irt5, eval=FALSE}
head(sapa)
```

```{r irt6, echo=FALSE, R.options = list(width = 110)}
sapa <- read.csv(paste0(getwd(), "/data_and_codes/sapa_data.csv"), header = TRUE)
head(sapa)
```

We can also see the names and types of the variables in our dataset using the `str()` function:

```{r irt7, echo=TRUE}
str(sapa)
```

The dataset consists of 1525 rows (i.e., examinees) and 16 variables (i.e., SAPA items). We can get more information on the dataset using the `introduce()` and `plot_intro()` functions from the **DataExplorer** package [@R-DataExplorer]:

```{r irt8, echo=TRUE, eval=FALSE}
DataExplorer::introduce(sapa)

DataExplorer::plot_intro(sapa)
```

```{r irt9, echo=FALSE}
kbl(t(introduce(sapa)), 
    row.names = TRUE, col.names = "", 
    format.args = list(big.mark = ",")) %>%
  kable_styling()
```

```{r irt10, echo=FALSE}
DataExplorer::plot_intro(sapa)
```

The plot above shows that all of the variables in the dataset are continuous. We also see that some of the variables have missing values but the proportion of missing data is very small (only 0.10%). To have a closer look at missing values, we can visualize the proportion of missingness for each variable using `plot_missing()` from **DataExplorer**.

```{r irt11, echo=TRUE}
DataExplorer::plot_missing(sapa)
```

To obtain a detailed summary of the sapa dataset, we will use the `describe()` function from the **psych** package [@R-psych].

```{r irt12, R.options = list(width = 200)}
psych::describe(x = sapa)
```

From the output above, we can see the number of individuals who responded to each SAPA item, the mean response value (i.e., proportion-correct or item difficulty), and other descriptive statistics. We see that most SAPA items have moderate difficulty values although the rotation items (i.e., rotate.3, rotate.4, rotate.6, and rotate.8) are more difficult than the remaining items in the dataset. 

In the [CTT](https://okanbulut.github.io/edpy507/ctt.html) section, we checked the correlations among the nfc items to gauge how strongly the items were associated with each other. We expected the items to be associated with each other because they were designed to measure the same latent trait (i.e., need for cognition). For the sapa dataset, we will have to make a similar assumption: all SAPA items measure the same latent trait (general intelligence or g). However, given that the items come from different content areas (i.e., verbal reasoning, letter series, matrix reasoning, and spatial rotations), we must ensure that these items are sufficiently correlated with each other.  

To compute the correlations among the SAPA items, we will use the `tetrachoric()` function from **psych**. Since the SAPA items are dichotomously scored (i.e., 0: incorrect and 1: correct), we cannot use Pearson correlations (which could be obtained using the `cor()` function in R). We will compute the correlations and then extract "rho" (i.e., the correlation matrix of the items).

```{r irt13, eval=FALSE}
# Save the correlation matrix
cormat <- psych::tetrachoric(x = sapa)$rho

# Print the correlation matrix
print(cormat)
```


```{r irt14, echo=FALSE}
cormat <- psych::tetrachoric(x = sapa)$rho

cormat %>%
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), font_size = 11)
```

The correlation matrix does not show any negative or low correlations (which is a very good sign! `r emo::ji("+1")`). To check the associations among the items more carefully, we will also create a correlation matrix plot using the `ggcorrplot()` function from the **ggcorrplot** package [@R-ggcorrplot]. We will include the `hc.order = TRUE` argument to perform hierarchical clustering. This will look for groups (i.e., clusters) of items that are strongly associated with each other. If all SAPA items measure the same latent trait, we should see a single cluster of items.

```{r irt15, echo=TRUE, fig.width=9, fig.height=6}
ggcorrplot::ggcorrplot(corr = cormat, # correlation matrix
                       type = "lower", # print only the lower part of the correlation matrix
                       hc.order = TRUE, # hierarchical clustering
                       show.diag = TRUE, # show the diagonal values of 1
                       lab = TRUE, # add correlation values as labels
                       lab_size = 3) # Size of the labels
```

The figure above shows that the four rotation items have created a cluster (see the cluster on the top-right corner), while the remaining SAPA items have created another cluster (see the cluster on the bottom-left corner). The rotation items are strongly correlated with each other (not surprising given that they all focus on the rotation skills); however, the same items have relatively lower correlations with the other items in the dataset. Also, matrix.55 seems to have relatively low correlations with the items from both clusters. 

Findings of hierarchical clustering suggest that the SAPA items may not be measuring a single latent trait. However, hierarchical clustering is not a test of dimensionality. To ensure that there is a single factor (i.e., latent trait) underlying the SAPA items, we need to perform factor analysis and evaluate the factor structure of the SAPA items (i.e., dimensionality).

<br>

### Exploratory factor analysis 

Factor analysis is a statistical modeling technique that aims to explain the common variability among a set of observed variables and transform the variables into a reduced set of variables known as factors (or dimensions). At the core of factor analysis is the desire to reduce the dimensionality of the data from $p$ observed variables to $q$ factors, such that $q < p$. During instrument development or when there are no prior beliefs about the dimensionality or structure of an existing instrument, exploratory factor analysis (EFA) should be considered to investigate the factorial structure of the instrument. To perform EFA, we need to specify the number of factors to extract, how to rotate factors (if the number of factors > 1), and which type of estimation method should be used depending on the nature of the data (e.g., categorical vs. continuous variables).

The **psych** package includes several functions to perform factor analytic analysis with different estimation methods. We will use the `fa()` function in the **psych** package to perform EFA. To use the function, we need to specify the following items:

* r = Response data (either raw data or a correlation matrix)
* n.obs = Number of observations in the data (necessary only when using a correlation matrix)
* nfactors = Number of factors that we expect to find in the data
* rotate = Type of rotation if $n > 1$. We can use "varimax" for an orthogonal rotation that assumes no correlation between factors or "oblimin" for an oblique rotation that assumes factors are somewhat correlated
* fm = Factor analysis method. We will use "pa" (i.e., principal axis) for EFA
* cor = How to find the correlations when using raw data. For continuous variable, use `cor = "Pearson"` (Pearson correlation); for dichotomous variables, use `cor = "tet"` (tetrachoric correlation); for polytomous variables (e.g., Likert scales), use `cor = "poly"` (polychoric correlation).

First, we will try a one-factor model, evaluate model fit, and determine whether a one-factor (i.e., unidimensional) structure is acceptable for the SAPA items:

```{r irt16, echo=TRUE}
# Try one-factor EFA model --> nfactors = 1
efa.model1 <- psych::fa(r = sapa, nfactors = 1, fm = "pa", cor = "tet")

# Print the results 
print(efa.model1, sort = TRUE) # Show the factor loadings sorted by absolute value
```

The output shows the factor loadings for each item (see the `PA1` column) and the proportion of explained variance (42\%; see `Proportion Var`). The factor loadings seem fine (i.e., > 0.3), which is the typical cut-off value to determine significant loadings. We can determine model fit based on model fit indices of root mean square of residuals (RMSR), root mean square error of approximation (RMSEA), and Tucker-Lewis Index. We can use @hu1999cutoff's guidelines for these model fit indices: Tucker-Lewis index (TLI) > .95, RMSEA < .06, and RMSR near zero indicate good model fit. The fit measures in the output show that the one-factor model does not necessarily fit the sapa dataset. Therefore, we will try a two-factor model in the next run: 

```{r irt17, echo=TRUE}
# Try two-factor EFA model --> nfactors=2
efa.model2 <- psych::fa(sapa, nfactors = 2, rotate = "oblimin", fm = "pa", cor = "tet")

# Print the results 
print(efa.model2, sort = TRUE) # Show the factor loadings sorted by absolute value
```

Based on the factor loadings listed under the PA1 and PA2 columns, we see that the first 12 items are highly loaded on the first factor whereas the last four items (i.e., rotation items) are highly loaded on the second factor. This finding is aligned with what we have observed in the correlation matrix plot earlier. Another important finding is that one of the items (matrix.55) is not sufficiently loaded on either of the two factors. 

The rest of the output shows that the first factor explains 30\% of the total variance while the second factor explains 19\% of the total variance (see `Proportion Var`). Compared to the one-factor model, the two-factor model explains an additional 7\% of variance in the data. The two factors seem to be moderately correlated ($r = .63$). The model fit indices show that the two-factor model fits the data better (though the model fit indices do not entirely meet the guidelines).

At this point, we need to make a theoretical decision informed by the statistical output: Can we still assume that all the items in the sapa dataset measure the same latent trait? Or, should we exclude the items that do not seem to correlate well with the rest of the items in the dataset? The evidence we obtained from the EFA models suggests that the rotation items may not be the part of the construct measured by the rest of the SAPA items. Also, matrix.55 appears to be a bit problematic. Therefore, we can choose to exclude these five items from the dataset.

In the following section, we will first use the `subset()` function (from base R) to drop the rotation items and matrix.55 and save the new dataset as "sapa_clean". Next, we will run the one-factor EFA model again using the items in sapa_clean.

```{r irt18, echo=TRUE}
# Drop the problematic items
sapa_clean <- subset(sapa, select = -c(rotate.3, rotate.4, rotate.6, rotate.8, matrix.55))

# Try one-factor EFA model with the clean dataset
efa.model3 <- psych::fa(sapa_clean, nfactors = 1, fm = "pa", cor = "tet")
print(efa.model3, sort=TRUE)
```

The output above shows that the model fit has improved significantly after removing the rotation items and matrix.55 from the dataset. Thus, we will use the sapa_clean dataset for subsequent analyses. Please note that for the sake of brevity, we followed a data-driven approach to determine whether the problematic items need to be removed from the data. A more suitable solution would be to review the content of these items carefully and the output of the EFA models, and make a decision considering both the theoretical assumptions regarding the items and the statistical findings. 

<br>

### Confirmatory factor analysis 

After an instrument has been developed and validated, we have a sense of its dimensionality (i.e., factor structure) and which observed variables (i.e., items) should load onto which factor(s). In this setting, it is more appropriate to consider confirmatory factor analysis (CFA) for examining the factor structure. Unlike with EFA, in CFA the researcher must create a theoretically-justified factor model by specifying the factor(s) and which items are associated with each factor and evaluate its fit to the data. 

Following the results of EFA from the previous section, we will go ahead and fit a one-factor CFA model to the sapa_clean dataset. To perform CFA in R, as well as path analysis and structural equation modeling, we can use the **lavaan** package [@R-lavaan], which stands for **la**tent **va**riable **an**alysis. The **lavaan** package uses its own special model syntax. For conducting a CFA, we need to define a model and then estimate the model using the `cfa()` function. The model definition below begins with a single quote and ends with the same single quote. We named our factor as "f" (or, we could name it as "intelligence") and listed the items associated with this factor (i.e., SAPA items). 

```{r irt19, echo=TRUE}
# Define a single factor
sapa_model <- 'f =~ reason.4 + reason.16 + reason.17 + reason.19 + letter.7 + 
               letter.33 + letter.34 + letter.58 + matrix.45 + matrix.46 + matrix.47'
```

Next, we will run the CFA model for the model defined above. If the items are either dichotomous or polytomous, then the estimator should be either "MLR" or "WLSMV" because these estimators are more robust against non-normality, which is usually the case for categorical data. In this example, we will use "MLR" (i.e., Robust Maximum Likelihood) to estimate our CFA model. 

```{r irt20a, echo=TRUE}
# Estimate the model
cfa_sapa <- lavaan::cfa(sapa_model, data = sapa_clean, estimator = "MLR")

# Print the output
summary(cfa_sapa, fit.measures=TRUE, standardized = TRUE)
```

The `cfa()` function returns a long output with model fit statistics, model parameters, and additional information, but we will only focus on model fit indices of Comparative Fit Index (CFI), Tucker-Lewis Index (TLI), and Root Mean Square Error of Approximation (RMSEA) to interpret the fit of the one-factor model to the sapa_clean dataset (please refer to [UCLA Statistical Consulting Group's website](https://stats.oarc.ucla.edu/r/seminars/rcfa/) for a more detailed coverage of CFA model estimation using **lavaan**. The website also includes annotated examples of a variety of statistical analyses using different software programs such as SPSS, SAS, R, and Mplus). 

In the output, both CFI and TLI values (under the "Robust" column) are larger than .95, indicating good model fit. Similarly, the RMSEA value of .042 for the one-factor model suggests good model fit (since it is less than the suggested cut-off value of .06). Also, the "Std.all" column in the "Latent Variables: f =~" section shows standardized factor loadings for the items. We see that all the items in sapa_clean have a high factor loading (> 0.3), indicating an adequate relationship with the estimated factor.

<br>

### Item analysis

Before we start the IRT analysis, let's take a look at the items by running item analysis based on Classical Test Theory (CTT). This will allow us to have a final look at the items that we identified based on EFA and CFA and ensure that the response dataset is ready for IRT analysis. We will use the `alpha()` function from the **psych** package for running CTT-based item analysis.

```{r irt20b, echo=TRUE}
# Run the item analysis and save it as itemanalysis_psych
itemanalysis_psych <- psych::alpha(x = sapa_clean)

# Print the results
itemanalysis_psych
```

The output shows that the reliability is $\alpha = .81$, suggesting that the SAPA items have high internal consistency. In the "Reliability if an item is dropped" section, we see that removing any of the SAPA items does not necessarily improve the coefficient alpha, suggesting that all the items are essential. Lastly, in the "Item statistics" section, we can see that point-biserial correlations under the r.drop and r.cor columns are quite high (i.e., $> .20$), indicating that all the SAPA items can distinguish low- and high-achieving students adequately. 

<br>

## Item calibration {.tabset .tabset-fade .tabset-pills}

In this section, we will see how to estimate different types of dichotomous IRT models using the sapa_clean dataset. This process is known as "item calibration". We will calibrate the SAPA items using the following IRT models:

```{r irt21, eval=TRUE, echo=FALSE}
models <- data.frame(
  Model = c("Rasch", "1PL", "2PL", "3PL"),
  Description = c("Rasch Model", "One-Parameter Logistic Model", "Two-Parameter Logistic Model", 
                  "Three-Parameter Logistic Model"),
  Parameters = c("b", "b, a (same for all items)", "b, a (unique to each item)", "b, a (unique to each item), c")
)

kbl(models, caption = "") %>%
  kable_paper("striped")
```

<br>

By clicking on each of the following tabs, you can see the estimation of item parameters for the four IRT models listed above. 

<br>

### Rasch Model

In the Rasch model, the probability of answering item $i$ correctly for examinee $j$ with ability $\theta_j$ (i.e., $P(X_{ij} = 1)$) can be written as follows:

$$P(X_{ij} = 1) = \frac{e^{(\theta_j - b_i)}}{1 + e^{(\theta_j - b_i)}}$$

where $b_i$ is the item difficulty level of item $i$. Using the Rasch model, we can estimate a unique difficulty parameter for each item and an ability parameter for each examinee. 

Now, let's calibrate the items in the sapa_clean dataset using the Rasch model.

```{r irt22, echo=TRUE, eval=FALSE}
# Estimate the item parameters
model_rasch <- mirt::mirt(data = sapa_clean, # data with only item responses
                          model = 1, # 1 refers to the unidimensional IRT model
                          itemtype = "Rasch") # IRT model we want to use for item calibration
```

```{r irt23, echo=FALSE, eval=TRUE}
# Estimate the item parameters
model_rasch <- mirt::mirt(data = sapa_clean, # data with only item responses
                          model = 1, # 1 refers to the unidimensional IRT model
                          itemtype = "Rasch", # IRT model we want to use for item calibration
                          verbose = FALSE)
```

Next, we will extract the estimated item parameters using the `coef()` function and print them. 

```{r irt24, echo=TRUE}
# Extract the item parameters
param_rasch <- coef(model_rasch, # the model object with the estimated parameters
                    IRTpars = TRUE, # whether we want to get traditional IRT parameters
                    simplify = TRUE) # simplify the model output

# What is saved in this object?
str(param_rasch)

# It is a list with a bunch of stuff, but... we only want to keep the item parameters
param_rasch <- as.data.frame(param_rasch$items)

# Print the item parameters
param_rasch
```

In the output, we see that the a parameter is fixed to "1" for all items, the b parameters are uniquely estimated for each item, and the c parameter (shown as "g" as guessing) is fixed to zero for all items. The "u" column indicates the upper asymptote representing the maximum value of the probability of success (this parameter only matters for the 4-parameter logistic (4PL) model and yes, there is indeed a 4PL model... `r emo::ji("weary")`).

<br>

### 1PL Model

In the one-parameter logistic (1PL) model, the probability of answering item $i$ correctly for examinee $j$ with ability $\theta_j$ can be written as follows:

$$P(X_{ij} = 1) = \frac{e^{a(\theta_j - b_i)}}{1 + e^{a(\theta_j - b_i)}}$$

where $b_i$ is the item difficulty level of item $i$ and $a$ is the item discrimination parameter of item $i$ (though it is **not** unique to item $i$). Using the 1PL model, we can estimate a unique difficulty parameter for each item, a discrimination parameter fixed across all the items, and an ability parameter for each examinee. 

Now, we will calibrate the items in the sapa_clean dataset using the 1PL model. Estimating the 1PL model requires a special setup in the **mirt** package. We will use the two-parameter logistic (2PL) model but constrain the discrimination parameters to be fixed across all the items, which will give us the 1PL model (i.e., unique difficulty parameters but a fixed discrimination). Instead of using `model = 1` in the `mirt()` function, we will define a model where we indicate the latent variable (called "F" in the following example), how many items are defining this model (i.e., `F = 1-11`), and which parameters are to be constrained. We use `CONSTRAIN = (1-11, a1)` to estimate a single discrimination parameter (called "a1" in **mirt**) for all of the 11 items in the dataset. 

```{r irt25, echo=TRUE, eval=FALSE}
# Define the 1PL model explicitly
model <- "F = 1-11
          CONSTRAIN = (1-11, a1)"

# Estimate the item parameters
model_1PL <- mirt::mirt(data = sapa_clean, # data with only item responses
                        model = model, # our special model for 1PL
                        itemtype = "2PL") # IRT model we want to use for item calibration
```

```{r irt26, echo=FALSE, eval=TRUE}
# Define the 1PL model explicitly
model <- "F = 1-11
          CONSTRAIN = (1-11, a1)"

# Estimate the item parameters
model_1PL <- mirt::mirt(data = sapa_clean, # data with only item responses
                        model = model, # our special model for 1PL
                        itemtype = "2PL", # IRT model we want to use for item calibration
                        verbose = FALSE)
```

Next, we will extract the estimated item parameters using the `coef()` function and print them. 

```{r irt27, echo=TRUE}
# Extract the item parameters
param_1PL <- coef(model_1PL, # the model object with the estimated parameters
                  IRTpars = TRUE, # whether we want to get traditional IRT parameters
                  simplify = TRUE) # simplify the model output

# Only keep the item parameters
param_1PL <- as.data.frame(param_1PL$items)

# Print the item parameters
param_1PL
```

In the output, we see that the a parameter is estimated but it is fixed to "1.478" for all items, the b parameters are uniquely estimated for each item, the c parameter (i.e., g) is fixed to zero, and the upper asymptote (i.e., "u") is fixed to 1 for all items. If we compare the difficulty parameters from Rasch and 1PL, we can see that they are not necessarily the same. Estimating the discrimination parameter (instead of assuming $a = 1$) has changed the parameter estimates in the 1PL model. However, they are perfectly correlated. 

```{r irt28, echo=TRUE}
# Combine the difficulty parameters
b_pars <- data.frame(rasch = param_rasch$b,
                     onePL = param_1PL$b)

# Print the difficulty parameters
b_pars

# Are they correlated?
cor(b_pars$rasch, b_pars$onePL)

# Let's also plot them -- see that they are perfectly aligned on a diagonal line
plot(b_pars$rasch, b_pars$onePL, 
     xlab = "Difficulty (Rasch)", 
     ylab = "Difficulty (1PL)",
     main = "Rasch vs. 1PL Difficulty Parameters")
```

<br>

### 2PL Model

In the two-parameter logistic (2PL) model, the probability of answering item $i$ correctly for examinee $j$ with ability $\theta_j$ can be written as follows:

$$P(X_{ij} = 1) = \frac{e^{a_i(\theta_j - b_i)}}{1 + e^{a_i(\theta_j - b_i)}}$$

where $b_i$ is the item difficulty level of item $i$ and $a_i$ is the item discrimination parameter of item $i$. Using the 1PL model, we can estimate a unique difficulty parameter and a unique discrimination parameter for each item, and an ability parameter for each examinee. 

Let's calibrate the items in the sapa_clean dataset using the 2PL model. Fortunately, estimating the 2PL model does not require any special set up. We will simply define itemtype as "2PL" and estimate the parameters as we have done for the Rasch model. 

```{r irt29, echo=TRUE, eval=FALSE}
# Estimate the item parameters
model_2PL <- mirt::mirt(data = sapa_clean, # data with only item responses
                        model = 1, # 1 refers to the unidimensional IRT model
                        itemtype = "2PL") # IRT model we want to use for item calibration
```

```{r irt30, echo=FALSE, eval=TRUE}
model_2PL <- mirt::mirt(data = sapa_clean, # data with only item responses
                        model = 1, # 1 refers to the unidimensional IRT model
                        itemtype = "2PL", # IRT model we want to use for item calibration
                        verbose = FALSE)
```

Next, we will extract the estimated item parameters using the `coef()` function and print them. 

```{r irt31, echo=TRUE}
# Extract the item parameters
param_2PL <- coef(model_2PL, # the model object with the estimated parameters
                  IRTpars = TRUE, # whether we want to get traditional IRT parameters
                  simplify = TRUE) # simplify the model output

# Only keep the item parameters
param_2PL <- as.data.frame(param_2PL$items)

# Print the item parameters
param_2PL
```

In the output, we see that the a and b parameters are uniquely estimated for each item, the c parameter (i.e., g) is fixed to zero, and the upper asymptote (i.e., "u") is fixed to 1 for all items. The discrimination parameters seem to vary across the items (e.g., $a = 1.816$ for reason.17 and $a = 1.112$ for matrix.45). Let's see whether the difficulty parameters from the 1PL and 2PL models are similar:


```{r irt32, echo=TRUE}
# Combine the difficulty parameters
b_pars <- data.frame(onePL = param_1PL$b,
                     twoPL = param_2PL$b)

# Print the difficulty parameters
b_pars

# Are they correlated? Yes, they are!
cor(b_pars$onePL, b_pars$twoPL)
```

<br>

### 3PL Model

In the three-parameter logistic (3PL) model, the probability of answering item $i$ correctly for examinee $j$ with ability $\theta_j$ can be written as follows:

$$P(X_{ij} = 1) = c_i + (1 - c_i)\frac{e^{a_i(\theta_j - b_i)}}{1 + e^{a_i(\theta_j - b_i)}}$$

where $b_i$ is the item difficulty level of item $i$, $a_i$ is the item discrimination parameter of item $i$, and $c_i$ is the guessing parameter (i.e., lower asymptote) of item $i$. Using the 3PL model, we can estimate unique difficulty, discrimination, and guessing parameters for each item, and an ability parameter for each examinee. 

Let's calibrate the items in the sapa_clean dataset using the 3PL model. This time, we will define itemtype as "3PL" and then estimate the parameters. 

```{r irt33, echo=TRUE, eval=FALSE}
# Estimate the item parameters
model_3PL <- mirt::mirt(data = sapa_clean, # data with only item responses
                        model = 1, # 1 refers to the unidimensional IRT model
                        itemtype = "3PL") # IRT model we want to use for item calibration
```

```{r irt34, echo=FALSE, eval=TRUE}
model_3PL <- mirt::mirt(data = sapa_clean, # data with only item responses
                        model = 1, # 1 refers to the unidimensional IRT model
                        itemtype = "3PL", # IRT model we want to use for item calibration
                        verbose = FALSE)
```

Next, we will extract the estimated item parameters using the `coef()` function and print them. 

```{r irt35, echo=TRUE}
# Extract the item parameters
param_3PL <- coef(model_3PL, # the model object with the estimated parameters
                  IRTpars = TRUE, # whether we want to get traditional IRT parameters
                  simplify = TRUE) # simplify the model output

# Only keep the item parameters
param_3PL <- as.data.frame(param_3PL$items)

# Print the item parameters
param_3PL
```

In the output, we see that the a, b, and c parameters are uniquely estimated for each item, and the upper asymptote (i.e., "u") is fixed to 1 for all items. Overall, most SAPA items have very low guessing parameters. The largest guessing parameter belongs to matrix.46 ($c = 0.147$).

<br>

## Visualizing IRT models

Once we calibrate the items using a particular IRT model, we can also check the response functions for each item, as well as for the entire instrument, visually. In the following section, we will create item- and test-level visualizations for the SAPA items using the item parameters we have obtained from the IRT models.

Let's begin with the item characteristic curves (ICC). We will use `itemplot()` to see the ICC for a specific item (i.e., `item = 10` for creating an ICC for item 10). We use `type = "trace"` because ICCs are also known as tracelines. In the ICC, the x-axis shows the latent trait (i.e., $\theta$) and the y-axis shows the probability of answering the item correctly (ranging from 0 to 1).

```{r irt36, echo=TRUE}
# Item 10 - 1PL model
mirt::itemplot(model_1PL, 
               item = 10, # which item to plot
               type = "trace") # traceline (i.e., ICC)
```

If we want to plot ICCs for more than one item, then we can the `plot()` function. We need to specify for which items we want to create an ICC using the `which.items` argument:

```{r irt37, echo=TRUE, fig.height=5}
# ICCs for items 1, 3, and 5 in the 2PL model
plot(model_2PL, 
     type = "trace", 
     which.items = c(1, 3, 5)) # items to be plotted (i.e., their positions in the data)
```

Also, we can plot the ICCs for all the items together using the same `plot()` function. Deleting the `which.items` argument will return the ICCs for all the items in a single plot. Now, let's see the ICCs for all SAPA items using the 3PL model:

```{r irt38, echo=TRUE}
# All ICCs in 3PL model
plot(model_3PL, type = "trace")
```

Next, we will use the `plot()` function and `type = "score"` to obtain test characteristic curves or TCC (i.e., sum of the individual ICCs). In the TCC, the x-axis shows the latent trait (i.e., $\theta$) and the y-axis shows the expected (raw) score based on the entire instrument (ranging from 0 to the maximum raw score possible on the instrument).

```{r irt39, echo=TRUE}
plot(model_2PL, type = "score")
```

One of the most important concepts in IRT is "item information", which refers to the level of information a particular item can provide at each level of the latent trait. The higher the information, the better measurement accuracy. We can visualize the information level of an item using the item information function (IIF). Another related concept is the standard error (also known as the conditional standard error of measurement; cSEM), which can be calculated as the reciprocal of the square root of the amount of item information:

$$SE_i(\theta) = \frac{1}{\sqrt{(I_i(\theta)})},$$
where $I_i(\theta)$ is the information of item $i$ at the latent trait $\theta$ and $SE_i(\theta)$ is the standard error of item $i$ at the latent trait $\theta$.

In the following example, we will plot the IIF and SE together for item 1 based on the 2PL model. We will use the `itemplot()` function with the `type = "infoSE"` argument. Alternatively, we could use `type = "info"` or `type = "SE"` to plot the IIF and SE separately. In the following plot, the x-axis shows the latent trait (i.e., $\theta$), the y-axis on the left shows item information, and the y-axis on the right shows the standard error. 

```{r irt40, echo=TRUE}
mirt::itemplot(model_2PL, # IRT model that stores the item information
               item = 1, # which item to plot
               type = "infoSE") # plot type
```

We can also view the IIFs for all the items in a single plot using the `plot()` function with the `type = "infotrace"` argument:

```{r irt41, echo=TRUE}
plot(model_rasch, type = "infotrace")
```

In addition to IIF and SE at the item level, we can also plot the information and SE values at the test level. That is, we can see the test information function (TIF) and the SE across all the items. In the following example, we will use the `plot()` function with the `type = "infoSE"` argument to produce a single plot of TIF and SE (we can also use either "info" or "SE" to plot them separately). In the plot, the x-axis shows the latent trait (i.e., $\theta$), the y-axis on the left shows the test information, and the y-axis on the right shows the total standard error for the SAPA items.

```{r irt42, echo=TRUE}
# TIF and SE for the 3PL model
plot(model_3PL, type = "infoSE")
```

Another useful visualization for IRT models is the item fit plot. This visualization allows us to inspect the fit of each item to the selected IRT model by plotting the empirical values (i.e., observed responses) against the predicted values (i.e., responses expected by the selected IRT model). Using this plot, we can find poorly fitting items. We will use the `itemfit()` function and `empirical.plot` to indicate which item we want to plot. The following plots show that the observed values are more aligned with the ICC produced by the 3PL model compared with the ICC produced by the 1PL model, suggesting that item 1 fits the 3PL model better.

```{r irt43, echo=TRUE}
# Item fit plot for item 1 in the 1PL model
mirt::itemfit(model_1PL, empirical.plot = 1)

# Item fit plot for item 1 in the 3PL model
mirt::itemfit(model_3PL, empirical.plot = 1)
```

The final visualization tool that we will use is called item-person map (also known as the Wright map in the IRT literature). An item-person map displays the location of item difficulty parameters and ability parameters (i.e., latent trait estimates) along the same logit scale. Item-person maps are very useful for comparing the spread of the item difficulty parameters against the distribution of the ability parameters. The item difficulty parameters are ideally expected to cover the whole logit scale to measure all the ability parameters accurately. 

In the following example, we will use the `ggWrightMap()` function from the **ShinyItemAnalysis** package [@R-ShinyItemAnalysis] to draw an item-person map for the Rasch model. This function requires the ability parameters (i.e., latent trait estimates) and item difficulty parameters from the Rasch model. We will use `fscores()` from **mirt** to estimate ability parameters (see the next section for more details on ability estimation). The following item-person map shows that the SAPA items mostly cover the middle part of the ability (i.e, latent trait) distribution. There are not enough items to cover high-achieving examinees (i.e., $\theta > 1$) and low-achieving examinees (i.e., $\theta < -2$). The SAPA instrument needs more items (some difficult and some easy) to increase measurement accuracy for those examinee groups. 

```{r irt44, echo=TRUE}
# Estimate the ability parameters for Rasch and save as a vector
theta_rasch <- as.vector(mirt::fscores(model_rasch))

# Use the difficulty and ability parameters to create an item-person map
ShinyItemAnalysis::ggWrightMap(theta = theta_rasch,
                               b = param_rasch$b,
                               item.names = colnames(sapa_clean), # item names (optional)
                               color = "lightblue", # color of ability distribution (optional)
                               ylab.theta = "Latent Trait", # label for ability distribution (optional)
                               ylab.b = "Item Difficulty") # label for item difficulty (optional)
```

<br>

## Ability estimation

Since we already calibrated the SAPA items using the IRT models, we can go ahead and estimate ability (i.e., latent trait) parameters for the examinees using the `fscores()` function. We will use the expected a priori (EAP) method to estimate the ability parameters. Alternatively, we can use `method = "ML"` (i.e., the maximum likelihood estimation) but this may not return a valid ability estimate for examinees who answered all of the items correctly or incorrectly. In the following example, we will estimate the ability parameters based on the 3PL model. 

```{r irt45, echo=TRUE}
# Estimate ability parameters
theta_3PL <- mirt::fscores(model_3PL, # estimated IRT model
                           method = "EAP", # estimation method
                           full.scores.SE = TRUE) # return the standard errors

# See the estimated ability parameters
head(theta_3PL)

# See the distribution of the estimated ability parameters
# (i.e., first column) in theta_3PL
hist(theta_3PL[, 1], 
     xlab = "Theta", # label for the x axis
     main = "Ability Distribution") # title for the plot
```

## Reliability

For IRT models, item and test information are the main criteria for determining the accuracy of the measurement process. However, it is also possible to calculate a CTT-like reliability index to evaluate the overall measurement accuracy for IRT models. The IRT test reliability coefficient ($\rho_{xx'}$) can be defined as the ratio of the true score variance (i.e., variance of ability estimates) to the observed score variance (the sum of the variance of ability estimates and the average of squared standard errors):


$$\rho_{xx'} = \frac{Var(\hat{\theta})}{Var(\hat{\theta}) + SE(\hat{\theta})^2}$$

This is known as "empirical reliability". In the following example, we will use `empirical_rxx()` function to calculate the marginal reliability coefficient for the 3PL model. 

```{r irt46, echo=TRUE}
# Empirical reliability
mirt::empirical_rxx(theta_3PL)
```

<br>

## What if SAPA items were polytomous?

In this example, we used the SAPA items which were scored dichotomously (i.e., 1 for correct and 0 for incorrect). If the SAPA items were polytomous, how could we estimate the IRT parameters? The following R codes show how the SAPA items could be calibrated using the Partial Credit Model and the Graded Response Model via the `mirt()` function. As we have done for the Rasch model, we can use `itemtype = "Rasch"` to calibrate the items based on the Partial Credit Model because this model is the polytomous extension of the Rasch model. For the Graded Response Model, we need `itemtype = "graded"`. The R codes that we have used above for extracting item parameters, visualizing the response functions, and estimating ability parameters for the dichotomous IRT models can also used for the Partial Credit and Graded Response models. 

```{r irt47, echo=TRUE, eval=FALSE}
# Partial Credit Model (PCM)
model_pcm <- mirt::mirt(data = sapa_clean,
                        model = 1, 
                        itemtype = "Rasch")


# Graded Response Model (GRM)
model_grm <- mirt::mirt(data = sapa_clean,
                        model = 1, 
                        itemtype = "graded")
```

<br>

# References
